{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "# 配置OpenAI API\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
    ")\n",
    "\n",
    "# 加载bge embedding模型\n",
    "model_name = \"BAAI/bge-base-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 函数：从文本生成嵌入向量\n",
    "def generate_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs)\n",
    "        embeddings = embeddings.pooler_output\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "# 构建FAISS索引\n",
    "dimension = 768  # 假设 bge embedding 输出的向量维度为 768\n",
    "\n",
    "# 示例文档库 (可以根据需求替换成真实文档)\n",
    "documents = [\n",
    "    \"The Eiffel Tower is in Paris.\",\n",
    "    \"Python is a popular programming language.\",\n",
    "    \"The human brain consists of billions of neurons.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成文档嵌入并添加到FAISS索引\n",
    "document_embeddings = generate_embedding(documents)\n",
    "\n",
    "index = faiss.IndexFlatL2(dimension)  # 使用L2距离\n",
    "index.add(document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查询阶段：从用户输入文本生成嵌入并查找相关文档\n",
    "def retrieve_documents(query, k=2):\n",
    "    query_embedding = generate_embedding(query)\n",
    "    distances, indices = index.search(query_embedding, k)  # 返回最近的k个文档\n",
    "    return [(documents[i], distances[0][j]) for j, i in enumerate(indices[0])]\n",
    "\n",
    "# 生成阶段：使用ChatGPT生成回答\n",
    "def generate_answer(query, context):\n",
    "    prompt = f\"Given the following context: {context}\\n\\nAnswer the following question: {query}\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": prompt,\n",
    "          }\n",
    "        ],\n",
    "        model=\"gpt-4o\",\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# 主流程：RAG模型\n",
    "def rag_pipeline(query):\n",
    "    # Step 1: 检索相关文档\n",
    "    retrieved_docs = retrieve_documents(query)\n",
    "    context = \" \".join([doc for doc, _ in retrieved_docs])\n",
    "    \n",
    "    # Step 2: 使用ChatGPT生成回答\n",
    "    generated_answer = generate_answer(query, context)\n",
    "    \n",
    "    return generated_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例查询\n",
    "query = \"What is the Eiffel Tower and where is it located?\"\n",
    "answer = rag_pipeline(query)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
