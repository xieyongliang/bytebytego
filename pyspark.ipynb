{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Dataloader\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 读取数据\n",
    "df_spark = spark.read.csv(\"test_pyspark.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 假设我们需要这些特征作为模型输入：feature1, feature2, ... , featureN\n",
    "# 和目标变量：target\n",
    "df_spark = df_spark.select(\"feature1\", \"feature2\", \"featureN\", \"target\")\n",
    "\n",
    "# 展示预处理后的数据\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = spark.read.csv( \"daily_sales_*.csv\", header=True, inferSchema=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "sales_df.filter(F.isnull(sales_df['customer_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = sales_df.dropDuplicates([\"order_id\"]).fillna({\"customer_id\":0, \"amount\":0.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, sum, col, count\n",
    "\n",
    "transformed_df = cleaned_df.withColumn(\"sales_category\", \n",
    "                    when(col(\"amount\") > 1000, \"high\" ). \n",
    "                    when(col(\"amount\") > 500, \"medium\"). \n",
    "                    otherwise(\"low\")\n",
    "                ).groupBy(\"region\", \"sales_category\").agg(\n",
    "                    sum(\"amount\").alias( \"total_sales\"), \n",
    "                    count(\"*\").alias(\"order_count\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 5. 写入 Hive 分区表\n",
    "transformed_df.write \\\n",
    "    .partitionBy(\"region\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .saveAsTable(\"regional_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. 记录元数据（可选\n",
    "spark.sql(\"ANALYZE TABLE regional_sales COMPUTE STATISTICS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['region', 'sales_category', 'total_sales', 'order_count']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv ( \"salary.csv\", header=True, inferSchema=True, escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='c1', age=20, department='finance', salary=10),\n",
       " Row(id=2, name='c2', age=12, department='it', salary=15),\n",
       " Row(id=3, name='c3', age=32, department='finance', salary=12),\n",
       " Row(id=4, name='c4', age=18, department='it', salary=13)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例1: 列操作 + 过滤 \n",
    "from pyspark.sql.functions import col, upper \n",
    "\n",
    "transformed_df = df.select(\n",
    "    upper(col(\"name\")).alias(\"name_upper\"), \n",
    "    col(\"age\").cast(\"integer\"), \n",
    "    (col(\"salary\") * 1.1).alias(\"adjusted_salary\")\n",
    ").filter(col(\"age\") > 18 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name_upper='C1', age=20, adjusted_salary=11.0),\n",
       " Row(name_upper='C3', age=32, adjusted_salary=13.200000000000001)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例2: 聚合操作\n",
    "agg_df = df.groupBy( \"department\") \\\n",
    "    .agg({\"salary\" : \"avg\", \"id\" : \"count\"}) \\\n",
    "    .withColumnRenamed(\"avg(salary)\", \"avg_salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(department='finance', count(id)=2, avg_salary=11.0),\n",
       " Row(department='it', count(id)=2, avg_salary=14.0)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = np.random.randn(100, 4)\n",
    "df = pd.DataFrame(data, dtype=np.float32, columns=['feature1', 'feature2', 'featureN', 'target'])\n",
    "df.to_csv('test_pyspark.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 Spark DataFrame 转换为 Pandas DataFrame\n",
    "#df_pandas = df_spark.toPandas()\n",
    "df_pandas = df\n",
    "\n",
    "# 将特征和标签分别提取\n",
    "X = df_pandas[[\"feature1\", \"feature2\", \"featureN\"]].values\n",
    "y = df_pandas[\"target\"].values\n",
    "\n",
    "# 将特征和标签转换为 PyTorch 张量\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# 创建数据集对象\n",
    "dataset = CustomDataset(X_tensor, y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 打印批次数据\n",
    "for batch_X, batch_y in dataloader:\n",
    "    print(batch_X.shape, batch_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义一个简单的模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# 实例化模型\n",
    "model = SimpleModel(input_dim=X_tensor.shape[1])\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in dataloader:\n",
    "        print(batch_X.shape, batch_y.shape)\n",
    "        # 前向传播\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y.unsqueeze(1))\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
